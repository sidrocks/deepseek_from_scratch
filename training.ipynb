{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Training DeepSeek Model from Scratch\n",
                "\n",
                "This notebook demonstrates how to train the custom `DeepSeek` model (imported from `model.py`) on custom dataset. It includes:\n",
                "1. Training for 10000 steps.\n",
                "2. Generating 5 sample ouputs\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found existing installation: torch 2.5.1+cu121\n",
                        "Uninstalling torch-2.5.1+cu121:\n",
                        "  Successfully uninstalled torch-2.5.1+cu121\n",
                        "Found existing installation: torchvision 0.20.1+cu121\n",
                        "Uninstalling torchvision-0.20.1+cu121:\n",
                        "  Successfully uninstalled torchvision-0.20.1+cu121\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\sidhe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~.rch'.\n",
                        "You can safely remove it manually.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
                        "Collecting torch\n",
                        "  Using cached https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-win_amd64.whl (2449.3 MB)\n",
                        "Collecting torchvision\n",
                        "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-win_amd64.whl (6.1 MB)\n",
                        "Requirement already satisfied: filelock in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.20.0)\n",
                        "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
                        "Requirement already satisfied: networkx in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.5)\n",
                        "Requirement already satisfied: jinja2 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
                        "Requirement already satisfied: fsspec in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.10.0)\n",
                        "Requirement already satisfied: setuptools in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
                        "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
                        "Requirement already satisfied: numpy in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (2.3.4)\n",
                        "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.4.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
                        "Installing collected packages: torch, torchvision\n",
                        "Successfully installed torch-2.5.1+cu121 torchvision-0.20.1+cu121\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
                        "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: transformers in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.57.1)\n",
                        "Requirement already satisfied: datasets in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.4.1)\n",
                        "Requirement already satisfied: filelock in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.20.0)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.36.0)\n",
                        "Requirement already satisfied: numpy>=1.17 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.3.4)\n",
                        "Requirement already satisfied: packaging>=20.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (25.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.3)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2025.11.3)\n",
                        "Requirement already satisfied: requests in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.5)\n",
                        "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.22.1)\n",
                        "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.7.0)\n",
                        "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
                        "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (22.0.0)\n",
                        "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.4.0)\n",
                        "Requirement already satisfied: pandas in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.3.3)\n",
                        "Requirement already satisfied: httpx<1.0.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.28.1)\n",
                        "Requirement already satisfied: xxhash in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.6.0)\n",
                        "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.18)\n",
                        "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
                        "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
                        "Requirement already satisfied: anyio in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
                        "Requirement already satisfied: certifi in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
                        "Requirement already satisfied: httpcore==1.* in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
                        "Requirement already satisfied: idna in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
                        "Requirement already satisfied: h11>=0.16 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
                        "Requirement already satisfied: colorama in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
                        "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
                        "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
                        "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
                        "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
                        "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
                        "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
                        "Requirement already satisfied: six>=1.5 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
                        "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sidhe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
                        "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
                    ]
                }
            ],
            "source": [
                "# Install PyTorch with CUDA support for Windows (assuming CUDA 12.1)\n",
                "# If this fails or you have a different CUDA version, check https://pytorch.org/get-started/locally/\n",
                "!pip uninstall -y torch torchvision\n",
                "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
                "!pip install transformers datasets"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> **IMPORTANT**: After running the cell above, you **MUST** restart the Jupyter Kernel for the changes to take effect. Go to **Kernel > Restart Kernel** in the menu."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PyTorch Version: 2.5.1+cu121\n",
                        "CUDA Available: True\n",
                        "CUDA Device: NVIDIA RTX 5000 Ada Generation Laptop GPU\n",
                        "CUDA Version: 12.1\n",
                        "Using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from torch.utils.data import DataLoader\n",
                "from transformers import AutoTokenizer, AutoConfig\n",
                "from datasets import load_dataset\n",
                "from model import DeepSeekLM  # Import our custom model\n",
                "import os\n",
                "\n",
                "print(f\"PyTorch Version: {torch.__version__}\")\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "if device == \"cpu\":\n",
                "    print(\"WARNING: You are running on CPU. Training will be very slow. Please ensure you have a GPU and the correct PyTorch version installed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Model and Tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Custom tokenizer not found at ./custom_tokenizer1.\n",
                        "Falling back to default tokenizer (HuggingFaceTB/SmolLM2-135M)...\n",
                        "Padding token set to: <|endoftext|>\n",
                        "Model vocab size updated to: 49152\n",
                        "DeepSeek Architecture Configuration:\n",
                        "Q LoRA Rank: 128\n",
                        "KV LoRA Rank: 128\n",
                        "NoPE Head Dim: 32\n",
                        "RoPE Head Dim: 32\n",
                        "Num Shared Experts: 1\n",
                        "Num Routed Experts: 8\n",
                        "Num Active Experts: 2\n",
                        "Expert Intermediate Size: 384\n",
                        "LlamaConfig {\n",
                        "  \"architectures\": [\n",
                        "    \"LlamaForCausalLM\"\n",
                        "  ],\n",
                        "  \"attention_bias\": false,\n",
                        "  \"attention_dropout\": 0.0,\n",
                        "  \"bos_token_id\": 0,\n",
                        "  \"dtype\": \"bfloat16\",\n",
                        "  \"eos_token_id\": 0,\n",
                        "  \"expert_intermediate_size\": 384,\n",
                        "  \"head_dim\": 64,\n",
                        "  \"hidden_act\": \"silu\",\n",
                        "  \"hidden_size\": 576,\n",
                        "  \"initializer_range\": 0.041666666666666664,\n",
                        "  \"intermediate_size\": 1536,\n",
                        "  \"is_llama_config\": true,\n",
                        "  \"kv_lora_rank\": 128,\n",
                        "  \"max_position_embeddings\": 8192,\n",
                        "  \"mlp_bias\": false,\n",
                        "  \"model_type\": \"llama\",\n",
                        "  \"nope_head_dim\": 32,\n",
                        "  \"num_active_experts\": 2,\n",
                        "  \"num_attention_heads\": 9,\n",
                        "  \"num_hidden_layers\": 30,\n",
                        "  \"num_key_value_heads\": 3,\n",
                        "  \"num_routed_experts\": 8,\n",
                        "  \"num_shared_experts\": 1,\n",
                        "  \"pretraining_tp\": 1,\n",
                        "  \"q_lora_rank\": 128,\n",
                        "  \"rms_norm_eps\": 1e-05,\n",
                        "  \"rope_head_dim\": 32,\n",
                        "  \"rope_interleaved\": false,\n",
                        "  \"rope_scaling\": null,\n",
                        "  \"rope_theta\": 100000,\n",
                        "  \"tie_word_embeddings\": true,\n",
                        "  \"transformers_version\": \"4.57.1\",\n",
                        "  \"use_cache\": true,\n",
                        "  \"vocab_size\": 49152\n",
                        "}\n",
                        "\n",
                        "Model initialized with DeepSeek Architecture.\n"
                    ]
                }
            ],
            "source": [
                "# Load Custom Tokenizer\n",
                "tokenizer_path = \"./custom_tokenizer1\"\n",
                "\n",
                "# Robust Tokenizer Loading\n",
                "if os.path.exists(tokenizer_path):\n",
                "    print(f\"Loading custom tokenizer from {tokenizer_path}\")\n",
                "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
                "else:\n",
                "    print(f\"Custom tokenizer not found at {tokenizer_path}.\")\n",
                "    print(\"Falling back to default tokenizer (HuggingFaceTB/SmolLM2-135M)...\")\n",
                "    model_id = \"HuggingFaceTB/SmolLM2-135M\"\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "\n",
                "# Ensure pad_token is set\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    print(f\"Padding token set to: {tokenizer.pad_token}\")\n",
                "\n",
                "model_id = \"HuggingFaceTB/SmolLM2-135M\"\n",
                "config = AutoConfig.from_pretrained(model_id)\n",
                "\n",
                "# Update config vocab size to match tokenizer\n",
                "config.vocab_size = len(tokenizer)\n",
                "print(f\"Model vocab size updated to: {config.vocab_size}\")\n",
                "\n",
                "# DeepSeek Architecture Configuration\n",
                "config.q_lora_rank = 128\n",
                "config.kv_lora_rank = 128\n",
                "config.nope_head_dim = 32\n",
                "config.rope_head_dim = 32\n",
                "config.num_shared_experts = 1\n",
                "config.num_routed_experts = 8\n",
                "config.num_active_experts = 2\n",
                "config.expert_intermediate_size = 384\n",
                "\n",
                "print(\"DeepSeek Architecture Configuration:\")\n",
                "print(f\"Q LoRA Rank: {config.q_lora_rank}\")\n",
                "print(f\"KV LoRA Rank: {config.kv_lora_rank}\")\n",
                "print(f\"NoPE Head Dim: {config.nope_head_dim}\")\n",
                "print(f\"RoPE Head Dim: {config.rope_head_dim}\")\n",
                "print(f\"Num Shared Experts: {config.num_shared_experts}\")\n",
                "print(f\"Num Routed Experts: {config.num_routed_experts}\")\n",
                "print(f\"Num Active Experts: {config.num_active_experts}\")\n",
                "print(f\"Expert Intermediate Size: {config.expert_intermediate_size}\")\n",
                "\n",
                "print(config)\n",
                "\n",
                "# Initialize model from scratch\n",
                "model = DeepSeekLM(config).to(device)\n",
                "print(\"Model initialized with DeepSeek Architecture.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prepare Dataset (Chunked)\n",
                "We concatenate text and split into chunks to allow the model to learn context across lines."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded local data from input-1.txt\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "207bc16983074a848522ff70b4e41a16",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "af30f2389faf4c77b0578dc34357e1e9",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset prepared. Number of chunks: 1332\n"
                    ]
                }
            ],
            "source": [
                "# Robust Data Loading\n",
                "data_file = \"input-1.txt\"\n",
                "if os.path.exists(data_file):\n",
                "    with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
                "        full_text = f.read()\n",
                "    print(f\"Loaded local data from {data_file}\")\n",
                "    from datasets import Dataset\n",
                "    dataset = Dataset.from_dict({\"text\": [full_text]})\n",
                "else:\n",
                "    print(f\"Data file {data_file} not found. Downloading default dataset from Hugging Face...\")\n",
                "    from datasets import load_dataset\n",
                "    # Use a small subset of a public dataset\n",
                "    dataset = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", split=\"train[:1%]\")\n",
                "    print(\"Loaded default dataset from Hugging Face.\")\n",
                "\n",
                "# Double check tokenizer padding\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "block_size = 256 # Context window size\n",
                "\n",
                "def group_texts(examples):\n",
                "    # Concatenate all texts\n",
                "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
                "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
                "    \n",
                "    if total_length >= block_size:\n",
                "        total_length = (total_length // block_size) * block_size\n",
                "        \n",
                "    result = {\n",
                "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
                "        for k, t in concatenated_examples.items()\n",
                "    }\n",
                "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
                "    return result\n",
                "\n",
                "def tokenize_function(examples):\n",
                "    return tokenizer(examples[\"text\"])\n",
                "\n",
                "# Tokenize all text\n",
                "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
                "\n",
                "# Group into chunks\n",
                "lm_dataset = tokenized_dataset.map(\n",
                "    group_texts,\n",
                "    batched=True,\n",
                "    batch_size=1000,\n",
                ")\n",
                "\n",
                "lm_dataset = lm_dataset.with_format(\"torch\")\n",
                "\n",
                "# Create dataloader\n",
                "train_dataloader = DataLoader(lm_dataset, batch_size=16, shuffle=True,pin_memory=True)\n",
                "print(f\"Dataset prepared. Number of chunks: {len(lm_dataset)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "DeepSeekLM(\n",
                        "  (embed_tokens): Embedding(49152, 576)\n",
                        "  (layers): ModuleList(\n",
                        "    (0-29): 30 x Block(\n",
                        "      (self_attn): DeepSeekMLA(\n",
                        "        (kv_down_proj): Linear(in_features=576, out_features=128, bias=False)\n",
                        "        (kv_norm): RMSNorm()\n",
                        "        (w_uk): Linear(in_features=128, out_features=288, bias=False)\n",
                        "        (w_ur): Linear(in_features=128, out_features=288, bias=False)\n",
                        "        (w_uv): Linear(in_features=128, out_features=576, bias=False)\n",
                        "        (q_down_proj): Linear(in_features=576, out_features=128, bias=False)\n",
                        "        (q_norm): RMSNorm()\n",
                        "        (w_uq): Linear(in_features=128, out_features=288, bias=False)\n",
                        "        (w_qr): Linear(in_features=128, out_features=288, bias=False)\n",
                        "        (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
                        "      )\n",
                        "      (mlp): DeepSeekMoE(\n",
                        "        (shared_experts): ModuleList(\n",
                        "          (0): DeepSeekExpertLayer(\n",
                        "            (gate_proj): Linear(in_features=576, out_features=384, bias=False)\n",
                        "            (up_proj): Linear(in_features=576, out_features=384, bias=False)\n",
                        "            (down_proj): Linear(in_features=384, out_features=576, bias=False)\n",
                        "            (act_fn): SiLU()\n",
                        "          )\n",
                        "        )\n",
                        "        (routed_experts): ModuleList(\n",
                        "          (0-7): 8 x DeepSeekExpertLayer(\n",
                        "            (gate_proj): Linear(in_features=576, out_features=384, bias=False)\n",
                        "            (up_proj): Linear(in_features=576, out_features=384, bias=False)\n",
                        "            (down_proj): Linear(in_features=384, out_features=576, bias=False)\n",
                        "            (act_fn): SiLU()\n",
                        "          )\n",
                        "        )\n",
                        "        (router): Linear(in_features=576, out_features=8, bias=False)\n",
                        "      )\n",
                        "      (input_layernorm): RMSNorm()\n",
                        "      (post_attention_layernorm): RMSNorm()\n",
                        "    )\n",
                        "  )\n",
                        "  (norm): RMSNorm()\n",
                        "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "print(model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Skipping torch.compile on Windows to avoid potential compatibility issues.\n",
                        "Starting training...\n",
                        "Step 100: Loss 4.6934 | Acc 0.2505 | TPS 4007.58\n",
                        "  Top Experts: [3, 1, 6] (Counts: [36809.0, 33519.0, 32624.0])\n",
                        "Step 200: Loss 4.0469 | Acc 0.3025 | TPS 4174.99\n",
                        "  Top Experts: [3, 1, 6] (Counts: [35844.0, 33733.0, 33348.0])\n",
                        "Step 300: Loss 3.7518 | Acc 0.3353 | TPS 4202.27\n",
                        "  Top Experts: [1, 3, 6] (Counts: [33874.0, 33787.0, 33483.0])\n",
                        "Step 400: Loss 3.4933 | Acc 0.3642 | TPS 4165.01\n",
                        "  Top Experts: [3, 1, 0] (Counts: [34359.0, 33296.0, 32160.0])\n",
                        "Step 500: Loss 3.1531 | Acc 0.4066 | TPS 4132.45\n",
                        "  Top Experts: [3, 1, 6] (Counts: [35348.0, 33343.0, 32253.0])\n",
                        "Step 600: Loss 2.2389 | Acc 0.5588 | TPS 4147.06\n",
                        "  Top Experts: [3, 6, 1] (Counts: [34546.0, 32690.0, 32171.0])\n",
                        "Step 700: Loss 1.9544 | Acc 0.6103 | TPS 4124.61\n",
                        "  Top Experts: [1, 3, 6] (Counts: [34124.0, 33445.0, 31511.0])\n",
                        "Step 800: Loss 1.4430 | Acc 0.7078 | TPS 4117.61\n",
                        "  Top Experts: [1, 6, 3] (Counts: [33659.0, 33184.0, 32558.0])\n",
                        "Step 900: Loss 0.9985 | Acc 0.7941 | TPS 4135.36\n",
                        "  Top Experts: [1, 3, 0] (Counts: [34557.0, 34180.0, 32137.0])\n",
                        "Step 1000: Loss 0.5874 | Acc 0.8880 | TPS 4159.42\n",
                        "  Top Experts: [3, 1, 0] (Counts: [33242.0, 33218.0, 32257.0])\n",
                        "\n",
                        "--- Step 1000 Generation ---\n",
                        "Generated: The meaning of life is now dead?\n",
                        "\n",
                        "NORTHUMBERLAND:\n",
                        "No, my lord, nothing but this shame,\n",
                        "At last I send me, though to seek loss,\n",
                        "Shall be the face, and look on Henry's life.\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 1100: Loss 0.1446 | Acc 0.9838 | TPS 4081.54\n",
                        "  Top Experts: [3, 1, 0] (Counts: [33201.0, 32888.0, 32215.0])\n",
                        "Step 1200: Loss 0.0846 | Acc 0.9863 | TPS 4097.88\n",
                        "  Top Experts: [3, 6, 1] (Counts: [32490.0, 32258.0, 31816.0])\n",
                        "Step 1300: Loss 0.0468 | Acc 0.9922 | TPS 4107.09\n",
                        "  Top Experts: [3, 1, 0] (Counts: [33978.0, 32864.0, 32625.0])\n",
                        "Step 1400: Loss 0.0262 | Acc 0.9958 | TPS 4120.77\n",
                        "  Top Experts: [6, 0, 3] (Counts: [32732.0, 32582.0, 31708.0])\n",
                        "Step 1500: Loss 0.0187 | Acc 0.9975 | TPS 4128.73\n",
                        "  Top Experts: [0, 1, 3] (Counts: [34485.0, 31924.0, 31604.0])\n",
                        "Step 1600: Loss 0.0168 | Acc 0.9966 | TPS 4142.95\n",
                        "  Top Experts: [3, 4, 1] (Counts: [31829.0, 31773.0, 31739.0])\n",
                        "Step 1700: Loss 0.0219 | Acc 0.9956 | TPS 4170.94\n",
                        "  Top Experts: [3, 1, 0] (Counts: [33449.0, 32459.0, 31572.0])\n",
                        "Step 1800: Loss 0.0122 | Acc 0.9980 | TPS 4196.24\n",
                        "  Top Experts: [3, 0, 1] (Counts: [33347.0, 32167.0, 31416.0])\n",
                        "Step 1900: Loss 0.0132 | Acc 0.9973 | TPS 4204.00\n",
                        "  Top Experts: [1, 3, 6] (Counts: [33147.0, 32207.0, 31234.0])\n",
                        "Step 2000: Loss 0.0177 | Acc 0.9961 | TPS 4208.44\n",
                        "  Top Experts: [1, 3, 4] (Counts: [32510.0, 31876.0, 31239.0])\n",
                        "\n",
                        "--- Step 2000 Generation ---\n",
                        "Generated: The meaning of life isle,\n",
                        "That by the justice of true judgment-ple'd out,\n",
                        "When heinous article, justice of what he lies\n",
                        "And what he bites, that must be set,\n",
                        "A sleeping England and lief-broke repeals,\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 2100: Loss 0.0169 | Acc 0.9971 | TPS 4171.85\n",
                        "  Top Experts: [1, 0, 4] (Counts: [8186.0, 7999.0, 7872.0])\n",
                        "Step 2200: Loss 0.0190 | Acc 0.9963 | TPS 4173.85\n",
                        "  Top Experts: [6, 3, 5] (Counts: [32350.0, 31431.0, 30997.0])\n",
                        "Step 2300: Loss 0.0452 | Acc 0.9941 | TPS 4176.96\n",
                        "  Top Experts: [1, 0, 3] (Counts: [33864.0, 32333.0, 31734.0])\n",
                        "Step 2400: Loss 0.3885 | Acc 0.9184 | TPS 4174.06\n",
                        "  Top Experts: [3, 4, 6] (Counts: [32086.0, 31306.0, 31299.0])\n",
                        "Step 2500: Loss 0.5854 | Acc 0.8588 | TPS 4179.81\n",
                        "  Top Experts: [3, 1, 6] (Counts: [32751.0, 32180.0, 31557.0])\n",
                        "Step 2600: Loss 0.1732 | Acc 0.9618 | TPS 4185.42\n",
                        "  Top Experts: [1, 4, 3] (Counts: [32360.0, 32266.0, 31473.0])\n",
                        "Step 2700: Loss 0.0365 | Acc 0.9939 | TPS 4180.68\n",
                        "  Top Experts: [6, 3, 2] (Counts: [32787.0, 32068.0, 31322.0])\n",
                        "Step 2800: Loss 0.0217 | Acc 0.9963 | TPS 4185.05\n",
                        "  Top Experts: [3, 4, 6] (Counts: [32959.0, 32066.0, 31313.0])\n",
                        "Step 2900: Loss 0.0198 | Acc 0.9958 | TPS 4192.67\n",
                        "  Top Experts: [0, 1, 4] (Counts: [31727.0, 31579.0, 31517.0])\n",
                        "Step 3000: Loss 0.0146 | Acc 0.9963 | TPS 4191.38\n",
                        "  Top Experts: [3, 6, 0] (Counts: [31895.0, 31704.0, 31341.0])\n",
                        "\n",
                        "--- Step 3000 Generation ---\n",
                        "Generated: The meaning of life isle'd,\n",
                        "In London by the tempest to this land of heaven:\n",
                        "Be not the holy sacrament, is the motive\n",
                        "Of this small inferior life.\n",
                        "\n",
                        "LEONTES:\n",
                        "Will't please your silence,--\n",
                        "\n",
                        "LE\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 3100: Loss 0.0102 | Acc 0.9980 | TPS 4130.37\n",
                        "  Top Experts: [4, 6, 1] (Counts: [32418.0, 32246.0, 31353.0])\n",
                        "Step 3200: Loss 0.0082 | Acc 0.9971 | TPS 4138.08\n",
                        "  Top Experts: [3, 6, 4] (Counts: [31501.0, 31420.0, 31036.0])\n",
                        "Step 3300: Loss 0.0107 | Acc 0.9968 | TPS 4145.70\n",
                        "  Top Experts: [3, 1, 2] (Counts: [31818.0, 31298.0, 31284.0])\n",
                        "Step 3400: Loss 0.0116 | Acc 0.9968 | TPS 4149.21\n",
                        "  Top Experts: [6, 3, 4] (Counts: [32225.0, 31946.0, 31304.0])\n",
                        "Step 3500: Loss 0.0075 | Acc 0.9980 | TPS 4152.85\n",
                        "  Top Experts: [3, 4, 0] (Counts: [32273.0, 31274.0, 31180.0])\n",
                        "Step 3600: Loss 0.0087 | Acc 0.9980 | TPS 4153.04\n",
                        "  Top Experts: [4, 6, 1] (Counts: [32157.0, 31398.0, 31264.0])\n",
                        "Step 3700: Loss 0.0093 | Acc 0.9973 | TPS 4152.48\n",
                        "  Top Experts: [6, 2, 1] (Counts: [32011.0, 32010.0, 31756.0])\n",
                        "Step 3800: Loss 0.0081 | Acc 0.9978 | TPS 4154.09\n",
                        "  Top Experts: [4, 1, 2] (Counts: [31715.0, 31666.0, 31351.0])\n",
                        "Step 3900: Loss 0.0162 | Acc 0.9958 | TPS 4158.75\n",
                        "  Top Experts: [1, 6, 0] (Counts: [31783.0, 31383.0, 31335.0])\n",
                        "Step 4000: Loss 0.0152 | Acc 0.9966 | TPS 4166.97\n",
                        "  Top Experts: [4, 1, 0] (Counts: [31832.0, 31728.0, 31423.0])\n",
                        "\n",
                        "--- Step 4000 Generation ---\n",
                        "Generated: The meaning of life isle,\n",
                        "In gross blood to this shame; till so be thou,\n",
                        "Thou art thou but the life of the life,\n",
                        "And this poor mortal company, this wretch,\n",
                        "Is it not wrong'd with the point of grief\n",
                        "\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 4100: Loss 0.0103 | Acc 0.9971 | TPS 4146.26\n",
                        "  Top Experts: [6, 3, 1] (Counts: [31363.0, 31254.0, 30904.0])\n",
                        "Step 4200: Loss 0.0107 | Acc 0.9980 | TPS 4146.69\n",
                        "  Top Experts: [1, 2, 3] (Counts: [8211.0, 8098.0, 8008.0])\n",
                        "Step 4300: Loss 0.0102 | Acc 0.9968 | TPS 4153.38\n",
                        "  Top Experts: [3, 6, 0] (Counts: [32714.0, 31615.0, 31491.0])\n",
                        "Step 4400: Loss 0.0095 | Acc 0.9968 | TPS 4157.83\n",
                        "  Top Experts: [6, 1, 2] (Counts: [32323.0, 31541.0, 31308.0])\n",
                        "Step 4500: Loss 0.0115 | Acc 0.9968 | TPS 4168.02\n",
                        "  Top Experts: [3, 4, 0] (Counts: [31788.0, 30926.0, 30906.0])\n",
                        "Step 4600: Loss 0.0109 | Acc 0.9971 | TPS 4180.91\n",
                        "  Top Experts: [1, 2, 3] (Counts: [31629.0, 31566.0, 31278.0])\n",
                        "Step 4700: Loss 0.0146 | Acc 0.9961 | TPS 4194.30\n",
                        "  Top Experts: [3, 0, 1] (Counts: [32227.0, 32100.0, 31092.0])\n",
                        "Step 4800: Loss 0.0080 | Acc 0.9973 | TPS 4198.21\n",
                        "  Top Experts: [6, 1, 2] (Counts: [31504.0, 31225.0, 31208.0])\n",
                        "Step 4900: Loss 0.0068 | Acc 0.9980 | TPS 4205.77\n",
                        "  Top Experts: [7, 6, 5] (Counts: [31519.0, 31469.0, 31307.0])\n",
                        "Step 5000: Loss 0.0136 | Acc 0.9966 | TPS 4215.92\n",
                        "  Top Experts: [6, 3, 5] (Counts: [31687.0, 31548.0, 31389.0])\n",
                        "\n",
                        "--- Step 5000 Generation ---\n",
                        "Generated: The meaning of life isle,\n",
                        "Of the sky-dro thee and haste\n",
                        "Of this vexation, when 'tis true that very hour\n",
                        "Than pity to it! no more: so look thee,\n",
                        "The precedent doth harbour such a nettle-\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 5100: Loss 0.0108 | Acc 0.9971 | TPS 4209.08\n",
                        "  Top Experts: [7, 3, 4] (Counts: [31259.0, 31062.0, 31060.0])\n",
                        "Step 5200: Loss 0.0145 | Acc 0.9958 | TPS 4214.50\n",
                        "  Top Experts: [3, 7, 1] (Counts: [32226.0, 31464.0, 31053.0])\n",
                        "Step 5300: Loss 0.0097 | Acc 0.9973 | TPS 4219.48\n",
                        "  Top Experts: [3, 1, 5] (Counts: [31887.0, 31766.0, 31334.0])\n",
                        "Step 5400: Loss 0.0080 | Acc 0.9975 | TPS 4226.18\n",
                        "  Top Experts: [6, 0, 7] (Counts: [32002.0, 31336.0, 31304.0])\n",
                        "Step 5500: Loss 0.0105 | Acc 0.9975 | TPS 4231.86\n",
                        "  Top Experts: [1, 3, 5] (Counts: [32079.0, 31140.0, 30991.0])\n",
                        "Step 5600: Loss 1.3267 | Acc 0.6716 | TPS 4233.19\n",
                        "  Top Experts: [0, 6, 3] (Counts: [32370.0, 31243.0, 31161.0])\n",
                        "Step 5700: Loss 0.6528 | Acc 0.8373 | TPS 4242.16\n",
                        "  Top Experts: [4, 0, 5] (Counts: [31643.0, 31510.0, 31047.0])\n",
                        "Step 5800: Loss 0.0447 | Acc 0.9897 | TPS 4245.72\n",
                        "  Top Experts: [4, 0, 6] (Counts: [32258.0, 31181.0, 31127.0])\n",
                        "Step 5900: Loss 0.0314 | Acc 0.9931 | TPS 4250.01\n",
                        "  Top Experts: [1, 0, 6] (Counts: [32239.0, 32009.0, 31601.0])\n",
                        "Step 6000: Loss 0.0184 | Acc 0.9953 | TPS 4254.31\n",
                        "  Top Experts: [4, 0, 1] (Counts: [31888.0, 31540.0, 31168.0])\n",
                        "\n",
                        "--- Step 6000 Generation ---\n",
                        "Generated: The meaning of life isle and mock you.\n",
                        "\n",
                        "MENENIUS:\n",
                        "If you be patient, I'll try how I'll try how\n",
                        "you means, power still without givingiest.\n",
                        "\n",
                        "BRUTUS:\n",
                        "Not in the party of your\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 6100: Loss 0.0144 | Acc 0.9961 | TPS 4246.98\n",
                        "  Top Experts: [0, 4, 1] (Counts: [33216.0, 31075.0, 30729.0])\n",
                        "Step 6200: Loss 0.0067 | Acc 0.9985 | TPS 4250.57\n",
                        "  Top Experts: [4, 7, 6] (Counts: [32680.0, 31546.0, 30947.0])\n",
                        "Step 6300: Loss 0.0242 | Acc 0.9951 | TPS 4251.02\n",
                        "  Top Experts: [5, 4, 0] (Counts: [8225.0, 8079.0, 8044.0])\n",
                        "Step 6400: Loss 0.0106 | Acc 0.9971 | TPS 4252.75\n",
                        "  Top Experts: [6, 1, 5] (Counts: [31966.0, 31162.0, 31098.0])\n",
                        "Step 6500: Loss 0.0118 | Acc 0.9968 | TPS 4248.87\n",
                        "  Top Experts: [4, 6, 5] (Counts: [32524.0, 31618.0, 31207.0])\n",
                        "Step 6600: Loss 0.0092 | Acc 0.9975 | TPS 4248.54\n",
                        "  Top Experts: [0, 4, 3] (Counts: [31807.0, 31462.0, 31192.0])\n",
                        "Step 6700: Loss 0.0095 | Acc 0.9973 | TPS 4248.98\n",
                        "  Top Experts: [2, 0, 6] (Counts: [32218.0, 31820.0, 31564.0])\n",
                        "Step 6800: Loss 0.0144 | Acc 0.9966 | TPS 4250.35\n",
                        "  Top Experts: [4, 6, 0] (Counts: [32110.0, 32046.0, 31726.0])\n",
                        "Step 6900: Loss 0.0103 | Acc 0.9971 | TPS 4250.97\n",
                        "  Top Experts: [3, 2, 7] (Counts: [31207.0, 31185.0, 31002.0])\n",
                        "Step 7000: Loss 0.0095 | Acc 0.9971 | TPS 4253.28\n",
                        "  Top Experts: [0, 6, 2] (Counts: [32347.0, 31590.0, 31396.0])\n",
                        "\n",
                        "--- Step 7000 Generation ---\n",
                        "Generated: The meaning of life isle and fruit fought,\n",
                        "And mark me we from this virtuous tears,\n",
                        "And you shall faint from wedlock judgment-like betroddenited.\n",
                        "This swells the siege of this fair prayer\n",
                        "And tell him that hath lost for bad\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 7100: Loss 0.0118 | Acc 0.9963 | TPS 4240.34\n",
                        "  Top Experts: [4, 3, 7] (Counts: [32111.0, 31613.0, 31485.0])\n",
                        "Step 7200: Loss 0.0104 | Acc 0.9966 | TPS 4241.01\n",
                        "  Top Experts: [6, 0, 7] (Counts: [32273.0, 31499.0, 31144.0])\n",
                        "Step 7300: Loss 0.0150 | Acc 0.9956 | TPS 4243.53\n",
                        "  Top Experts: [2, 0, 3] (Counts: [32130.0, 31461.0, 30738.0])\n",
                        "Step 7400: Loss 0.0057 | Acc 0.9978 | TPS 4245.77\n",
                        "  Top Experts: [4, 5, 6] (Counts: [31879.0, 31120.0, 31001.0])\n",
                        "Step 7500: Loss 0.0048 | Acc 0.9985 | TPS 4247.04\n",
                        "  Top Experts: [0, 1, 3] (Counts: [32428.0, 31633.0, 30684.0])\n",
                        "Step 7600: Loss 0.0107 | Acc 0.9973 | TPS 4250.12\n",
                        "  Top Experts: [4, 0, 2] (Counts: [31535.0, 31476.0, 31469.0])\n",
                        "Step 7700: Loss 0.0130 | Acc 0.9966 | TPS 4250.85\n",
                        "  Top Experts: [0, 6, 2] (Counts: [31968.0, 31786.0, 31377.0])\n",
                        "Step 7800: Loss 0.0162 | Acc 0.9958 | TPS 4253.67\n",
                        "  Top Experts: [3, 6, 4] (Counts: [32232.0, 30888.0, 30887.0])\n",
                        "Step 7900: Loss 0.0037 | Acc 0.9988 | TPS 4251.89\n",
                        "  Top Experts: [7, 5, 0] (Counts: [31259.0, 30906.0, 30851.0])\n",
                        "Step 8000: Loss 0.0100 | Acc 0.9968 | TPS 4251.39\n",
                        "  Top Experts: [6, 0, 4] (Counts: [32235.0, 31853.0, 31470.0])\n",
                        "\n",
                        "--- Step 8000 Generation ---\n",
                        "Generated: The meaning of life isle and record,\n",
                        "But that we oweed much strength to confutes.\n",
                        "\n",
                        "BUSHENRY PERCY:\n",
                        "My lord, I am going to the goal;\n",
                        "And long'd it is the issue:\n",
                        "This is the\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 8100: Loss 0.0092 | Acc 0.9968 | TPS 4240.68\n",
                        "  Top Experts: [0, 3, 7] (Counts: [32540.0, 31478.0, 31353.0])\n",
                        "Step 8200: Loss 0.0074 | Acc 0.9978 | TPS 4241.94\n",
                        "  Top Experts: [1, 6, 4] (Counts: [32136.0, 31805.0, 31606.0])\n",
                        "Step 8300: Loss 0.0118 | Acc 0.9963 | TPS 4089.25\n",
                        "  Top Experts: [1, 6, 2] (Counts: [32083.0, 31726.0, 31645.0])\n",
                        "Step 8400: Loss 0.0172 | Acc 0.9941 | TPS 3907.54\n",
                        "  Top Experts: [4, 2, 6] (Counts: [8092.0, 8054.0, 7937.0])\n",
                        "Step 8500: Loss 0.0102 | Acc 0.9968 | TPS 3765.16\n",
                        "  Top Experts: [4, 6, 2] (Counts: [31729.0, 31629.0, 31591.0])\n",
                        "Step 8600: Loss 0.0094 | Acc 0.9968 | TPS 3774.17\n",
                        "  Top Experts: [4, 1, 3] (Counts: [32896.0, 31350.0, 31273.0])\n",
                        "Step 8700: Loss 0.0123 | Acc 0.9971 | TPS 3781.50\n",
                        "  Top Experts: [5, 3, 7] (Counts: [31082.0, 31071.0, 31018.0])\n",
                        "Step 8800: Loss 0.0357 | Acc 0.9912 | TPS 3789.98\n",
                        "  Top Experts: [3, 6, 0] (Counts: [32023.0, 31713.0, 31389.0])\n",
                        "Step 8900: Loss 0.9243 | Acc 0.7471 | TPS 3795.72\n",
                        "  Top Experts: [0, 6, 1] (Counts: [32457.0, 31687.0, 31443.0])\n",
                        "Step 9000: Loss 0.1087 | Acc 0.9725 | TPS 3800.51\n",
                        "  Top Experts: [0, 6, 2] (Counts: [33510.0, 31714.0, 31015.0])\n",
                        "\n",
                        "--- Step 9000 Generation ---\n",
                        "Generated: The meaning of life isle:\n",
                        "But, for his own affections' means,\n",
                        "Which grieves to be suppliken!\n",
                        "\n",
                        "Second Murderer:\n",
                        "'Zounds, he does sit in the reward,\n",
                        "And not his house looker so\n",
                        "-----------------------------\n",
                        "\n",
                        "Step 9100: Loss 0.0328 | Acc 0.9929 | TPS 3796.31\n",
                        "  Top Experts: [0, 2, 7] (Counts: [32007.0, 31803.0, 31204.0])\n",
                        "Step 9200: Loss 0.0136 | Acc 0.9958 | TPS 3801.69\n",
                        "  Top Experts: [6, 0, 2] (Counts: [31495.0, 31460.0, 31325.0])\n",
                        "Step 9300: Loss 0.0168 | Acc 0.9953 | TPS 3807.41\n",
                        "  Top Experts: [5, 6, 0] (Counts: [32709.0, 31343.0, 31130.0])\n",
                        "Step 9400: Loss 0.0136 | Acc 0.9968 | TPS 3812.82\n",
                        "  Top Experts: [6, 1, 0] (Counts: [32269.0, 31817.0, 31136.0])\n",
                        "Step 9500: Loss 0.0120 | Acc 0.9961 | TPS 3816.78\n",
                        "  Top Experts: [2, 0, 1] (Counts: [31547.0, 31362.0, 30832.0])\n",
                        "Step 9600: Loss 0.0069 | Acc 0.9983 | TPS 3821.84\n",
                        "  Top Experts: [2, 0, 6] (Counts: [32781.0, 31838.0, 31271.0])\n",
                        "Step 9700: Loss 0.0129 | Acc 0.9963 | TPS 3826.12\n",
                        "  Top Experts: [1, 0, 3] (Counts: [31768.0, 31009.0, 30989.0])\n",
                        "Step 9800: Loss 0.0103 | Acc 0.9968 | TPS 3832.14\n",
                        "  Top Experts: [5, 0, 6] (Counts: [31817.0, 31542.0, 31316.0])\n",
                        "Step 9900: Loss 0.0133 | Acc 0.9963 | TPS 3836.35\n",
                        "  Top Experts: [6, 2, 1] (Counts: [31872.0, 31545.0, 31369.0])\n",
                        "Step 10000: Loss 0.0083 | Acc 0.9978 | TPS 3839.70\n",
                        "  Top Experts: [5, 4, 6] (Counts: [32753.0, 31491.0, 31143.0])\n",
                        "\n",
                        "--- Step 10000 Generation ---\n",
                        "Generated: The meaning of life isle of thee,\n",
                        "That thou wert possess'd Richard'st degree,\n",
                        "Is far off so happy by his pilgrimage;\n",
                        "And yet, madam, he is held forsworn.\n",
                        "\n",
                        "KING RICHARD II:\n",
                        "\n",
                        "-----------------------------\n",
                        "\n",
                        "Checkpoint saved to checkpoint_10000.pt\n",
                        "\n",
                        "--- Final Generations (5 Outputs) ---\n",
                        "\n",
                        "Generation 1:\n",
                        "Generated: The future of AI is his kin:\n",
                        "He is, my lords, to reap them to the Tower.\n",
                        "\n",
                        "NORTHUMBERLAND:\n",
                        "He hath been ta'en, and all the sky\n",
                        "The sland hath been breathing high,\n",
                        "Which often up Lancaster, to let him speak;\n",
                        "And therefore, if the duke sleep under him,\n",
                        "And give him not only.\n",
                        "\n",
                        "LORD ROSS:\n",
                        "It is excellent, gentle Warwick, and thou art\n",
                        "And all the duke\n",
                        "\n",
                        "Generation 2:\n",
                        "Generated: Once upon a time;\n",
                        "And what we are, that they shall be tears.\n",
                        "\n",
                        "HENRY BOLINGBROKE:\n",
                        "Thou counterfeit'st a woe to make me ask.\n",
                        "\n",
                        "KING RICHARD II:\n",
                        "Northumberland, thou hast resisted; the breath of care?\n",
                        "\n",
                        "DUKE OF AUMERLE:\n",
                        "Fitzwater, thou art all things to be satisfied.\n",
                        "\n",
                        "KING RICHARD II:\n",
                        "Thou art too careless\n",
                        "\n",
                        "Generation 3:\n",
                        "Generated: In a galaxy far away.\n",
                        "\n",
                        "THOMAS MOWBRAY:\n",
                        "Then thus I turn me from my country's light,\n",
                        "To dwell in solemn shades of endless night.\n",
                        "\n",
                        "KING RICHARD II:\n",
                        "Return again, and take an oath with thee.\n",
                        "Lay on our royal sword your banish'd hands;\n",
                        "Swear by the duty that you owe to God--\n",
                        "Our part therein we banish with yourselves--\n",
                        "To keep the oath that we administer:\n",
                        "You never shall\n",
                        "\n",
                        "Generation 4:\n",
                        "Generated: The secret to happiness is vain:\n",
                        "Thou art most likely; and not the noise of night\n",
                        "And all the swift passage polts,\n",
                        "Concern me, three shepherds looking on the night,\n",
                        "Like to the dead bodies of the devil's top,\n",
                        "And time the fadvised her friends':\n",
                        "Make motion through the sun under fiends roar'd.\n",
                        "This is the duke, that's gone roundly\n",
                        "To the dead bodies of the envious siege\n",
                        "Of watery Neptune, and not our\n",
                        "\n",
                        "Generation 5:\n",
                        "Generated: Python is a programming language that is Elys\n",
                        "Even in the best of your infant.\n",
                        "\n",
                        "HENRY BOLINGBROKE:\n",
                        "Thy pains appellant and thy worth;\n",
                        "But here Prince Florizel for my ground:\n",
                        "Had I been willing to lament the regal\n",
                        "The worst of my balmity with that time\n",
                        "I'ldominable to be punish'd.\n",
                        "\n",
                        "KING RICHARD II:\n",
                        "Dost thou attend me so? thou hast hawks will adder\n",
                        "-------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "# Optimization: Enable TF32 for faster matrix multiplications on Ampere+ GPUs\n",
                "torch.backends.cuda.matmul.allow_tf32 = True\n",
                "torch.backends.cudnn.allow_tf32 = True\n",
                "\n",
                "if os.name != 'nt':\n",
                "     print(\"Compiling model with torch.compile...\")\n",
                "     model = torch.compile(model)\n",
                "else:\n",
                "    print(\"Skipping torch.compile on Windows to avoid potential compatibility issues.\")\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, fused=True if torch.cuda.is_available() else False)\n",
                "loss_fn = torch.nn.CrossEntropyLoss()\n",
                "\n",
                "# Optimization: Mixed Precision Training\n",
                "scaler = torch.amp.GradScaler('cuda')\n",
                "\n",
                "def generate_text(model, tokenizer, prompt=\"The meaning of life is\", max_new_tokens=50, temperature=0.7, top_k=50):\n",
                "    model.eval()\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
                "    input_ids = inputs.input_ids\n",
                "    \n",
                "    for _ in range(max_new_tokens):\n",
                "        with torch.no_grad():\n",
                "            with torch.amp.autocast('cuda'):\n",
                "                logits = model(input_ids)\n",
                "                if isinstance(logits, tuple):\n",
                "                    logits = logits[0]\n",
                "            next_token_logits = logits[:, -1, :] / temperature\n",
                "            \n",
                "            top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
                "            probs = torch.nn.functional.softmax(top_k_logits, dim=-1)\n",
                "            next_token_index = torch.multinomial(probs, num_samples=1)\n",
                "            next_token = top_k_indices.gather(-1, next_token_index)\n",
                "            \n",
                "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
                "            \n",
                "            if next_token.item() == tokenizer.eos_token_id:\n",
                "                break\n",
                "            \n",
                "    print(f\"Generated: {tokenizer.decode(input_ids[0], skip_special_tokens=True)}\")\n",
                "    model.train()\n",
                "\n",
                "steps = 0\n",
                "max_steps = 10000\n",
                "save_path = \"checkpoint_10000.pt\"\n",
                "\n",
                "model.train()\n",
                "print(\"Starting training...\")\n",
                "\n",
                "import time\n",
                "start_time = time.time()\n",
                "total_tokens = 0\n",
                "\n",
                "# Loop indefinitely until max_steps is reached\n",
                "while steps < max_steps:\n",
                "    for batch in train_dataloader:\n",
                "        if steps >= max_steps:\n",
                "            break\n",
                "            \n",
                "        input_ids = batch[\"input_ids\"].to(device)\n",
                "        labels = input_ids.clone()\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        with torch.amp.autocast('cuda'):\n",
                "            outputs = model(input_ids)\n",
                "            if isinstance(outputs, tuple):\n",
                "                logits, expert_usage = outputs\n",
                "            else:\n",
                "                logits = outputs\n",
                "                expert_usage = None\n",
                "            \n",
                "            shift_logits = logits[..., :-1, :].contiguous()\n",
                "            shift_labels = labels[..., 1:].contiguous()\n",
                "            \n",
                "            loss = loss_fn(shift_logits.view(-1, config.vocab_size), shift_labels.view(-1))\n",
                "            \n",
                "            # Calculate Accuracy\n",
                "            with torch.no_grad():\n",
                "                preds = torch.argmax(shift_logits, dim=-1)\n",
                "                correct = (preds == shift_labels).sum()\n",
                "                total = shift_labels.numel()\n",
                "                accuracy = correct.float() / total\n",
                "        \n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        \n",
                "        steps += 1\n",
                "        total_tokens += input_ids.numel()\n",
                "        \n",
                "        if steps % 100 == 0:\n",
                "            elapsed = time.time() - start_time\n",
                "            tps = total_tokens / elapsed\n",
                "            print(f\"Step {steps}: Loss {loss.item():.4f} | Acc {accuracy.item():.4f} | TPS {tps:.2f}\")\n",
                "            if expert_usage is not None:\n",
                "                top_experts = torch.topk(expert_usage, k=3)\n",
                "                print(f\"  Top Experts: {top_experts.indices.tolist()} (Counts: {top_experts.values.tolist()})\")\n",
                "            \n",
                "        if steps % 1000 == 0:\n",
                "            print(f\"\\n--- Step {steps} Generation ---\")\n",
                "            generate_text(model, tokenizer)\n",
                "            print(\"-----------------------------\\n\")\n",
                "\n",
                "# Save Checkpoint\n",
                "torch.save(model.state_dict(), save_path)\n",
                "print(f\"Checkpoint saved to {save_path}\")\n",
                "\n",
                "print(\"\\n--- Final Generations (5 Outputs) ---\")\n",
                "prompts = [\"The future of AI is\", \"Once upon a time\", \"In a galaxy far away\", \"The secret to happiness is\", \"Python is a programming language that\"]\n",
                "for i, p in enumerate(prompts):\n",
                "    print(f\"\\nGeneration {i+1}:\")\n",
                "    generate_text(model, tokenizer, prompt=p, max_new_tokens=100)\n",
                "print(\"-------------------------------------\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
